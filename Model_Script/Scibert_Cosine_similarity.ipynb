{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b96a1c5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.037986,
     "end_time": "2022-01-15T06:24:44.218953",
     "exception": false,
     "start_time": "2022-01-15T06:24:44.180967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SciBERT + Cosine Similarity  on Neuroscience data     \n",
    "\n",
    "Apply pretrained SciBERT transformer model and Cosine Similarity for recommending reviewers who have published neuroscience research papers on semantically similar research topic as the user's input abstract query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dc7e5",
   "metadata": {
    "papermill": {
     "duration": 0.035371,
     "end_time": "2022-01-15T06:24:44.292640",
     "exception": false,
     "start_time": "2022-01-15T06:24:44.257269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Approach    \n",
    "- Load the pretrained SciBert model and tokenizer\n",
    "- Vectorize documents by creating embeddings\n",
    "- Semantic Similarity search by Cosine Similarity   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff79c6",
   "metadata": {
    "papermill": {
     "duration": 0.034689,
     "end_time": "2022-01-15T06:24:44.362817",
     "exception": false,
     "start_time": "2022-01-15T06:24:44.328128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c42ed2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T06:24:44.451983Z",
     "iopub.status.busy": "2022-01-15T06:24:44.450888Z",
     "iopub.status.idle": "2022-01-15T06:25:11.768999Z",
     "shell.execute_reply": "2022-01-15T06:25:11.768232Z",
     "shell.execute_reply.started": "2022-01-15T05:29:52.859368Z"
    },
    "papermill": {
     "duration": 27.370619,
     "end_time": "2022-01-15T06:25:11.769238",
     "exception": false,
     "start_time": "2022-01-15T06:24:44.398619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 KB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.8.1 tokenizers-0.12.1 transformers-4.20.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Hugging Face Transformer libraries\n",
    "!pip install transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer,  AutoModelForSequenceClassification\n",
    "\n",
    "# Similarity search: cosine similarity search \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d6cd6",
   "metadata": {
    "papermill": {
     "duration": 0.078219,
     "end_time": "2022-01-15T06:25:12.250621",
     "exception": false,
     "start_time": "2022-01-15T06:25:12.172402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a21f6ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T06:25:12.411987Z",
     "iopub.status.busy": "2022-01-15T06:25:12.409432Z",
     "iopub.status.idle": "2022-01-15T06:25:47.269807Z",
     "shell.execute_reply": "2022-01-15T06:25:47.270433Z",
     "shell.execute_reply.started": "2022-01-15T05:29:52.877923Z"
    },
    "papermill": {
     "duration": 34.942065,
     "end_time": "2022-01-15T06:25:47.270623",
     "exception": false,
     "start_time": "2022-01-15T06:25:12.328558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (3948, 5)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json(\"bioarxiv_parsed.json\") \n",
    "print(\"Data Shape: {}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b1945e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T06:25:47.427576Z",
     "iopub.status.busy": "2022-01-15T06:25:47.426521Z",
     "iopub.status.idle": "2022-01-15T06:25:47.512813Z",
     "shell.execute_reply": "2022-01-15T06:25:47.513401Z",
     "shell.execute_reply.started": "2022-01-15T05:30:10.836009Z"
    },
    "papermill": {
     "duration": 0.168791,
     "end_time": "2022-01-15T06:25:47.513581",
     "exception": false,
     "start_time": "2022-01-15T06:25:47.344790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       0.0\n",
       "abstract    0.0\n",
       "doi         0.0\n",
       "authors     0.0\n",
       "source      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of missing column values\n",
    "null_check_percent = data.isnull().sum() * 100 / len(data)\n",
    "null_check_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c6d961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T06:25:47.840058Z",
     "iopub.status.busy": "2022-01-15T06:25:47.833027Z",
     "iopub.status.idle": "2022-01-15T06:25:47.963934Z",
     "shell.execute_reply": "2022-01-15T06:25:47.964506Z",
     "shell.execute_reply.started": "2022-01-15T05:30:10.913682Z"
    },
    "papermill": {
     "duration": 0.218601,
     "end_time": "2022-01-15T06:25:47.964691",
     "exception": false,
     "start_time": "2022-01-15T06:25:47.746090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       0.0\n",
       "abstract    0.0\n",
       "doi         0.0\n",
       "authors     0.0\n",
       "source      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove articles with missing abstract\n",
    "data = data.dropna(subset = ['abstract'])\n",
    "data = data.reset_index(drop = True)\n",
    "null_check_percent = data.isnull().sum() * 100 / len(data)\n",
    "null_check_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d273e35",
   "metadata": {
    "papermill": {
     "duration": 0.079389,
     "end_time": "2022-01-15T06:25:48.300229",
     "exception": false,
     "start_time": "2022-01-15T06:25:48.220840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Pretrained SciBERT model  and tokenizer \n",
    "\n",
    "set the `output_hidden_states` to `True` so that we can extract the embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f5fa36e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T06:25:48.463138Z",
     "iopub.status.busy": "2022-01-15T06:25:48.462004Z",
     "iopub.status.idle": "2022-01-15T06:26:11.762010Z",
     "shell.execute_reply": "2022-01-15T06:26:11.763432Z",
     "shell.execute_reply.started": "2022-01-15T05:30:11.048060Z"
    },
    "papermill": {
     "duration": 23.385733,
     "end_time": "2022-01-15T06:26:11.763972",
     "exception": false,
     "start_time": "2022-01-15T06:25:48.378239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72671965cabb4036a684e3a6df058e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/223k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0534858794a14f8385cd2a016c9f59b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40bebf6c0dd474e82907093d6376ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/422M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Get the SciBERT pretrained model path from Allen AI repo\n",
    "pretrained_model = 'allenai/scibert_scivocab_uncased'\n",
    "\n",
    "# Get the tokenizer from the previous path\n",
    "sciBERT_tokenizer = BertTokenizer.from_pretrained(pretrained_model, \n",
    "                                          do_lower_case=True)\n",
    "\n",
    "# Get the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model,\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3a26f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ed583c",
   "metadata": {
    "papermill": {
     "duration": 0.138463,
     "end_time": "2022-01-15T06:26:12.044653",
     "exception": false,
     "start_time": "2022-01-15T06:26:11.906190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create an embedding for a given text data using SciBERT pre-trained model. \n",
    "\n",
    "Reference: [3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94e4f80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T06:26:12.300068Z",
     "iopub.status.busy": "2022-01-15T06:26:12.298921Z",
     "iopub.status.idle": "2022-01-15T06:26:12.302406Z",
     "shell.execute_reply": "2022-01-15T06:26:12.301792Z",
     "shell.execute_reply.started": "2022-01-15T05:30:16.767571Z"
    },
    "papermill": {
     "duration": 0.119839,
     "end_time": "2022-01-15T06:26:12.302565",
     "exception": false,
     "start_time": "2022-01-15T06:26:12.182726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_single_abstract_to_embedding(tokenizer, model, in_text, MAX_LEN = 510):\n",
    "    \n",
    "    input_ids = tokenizer.encode(\n",
    "                        in_text, \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = MAX_LEN,                           \n",
    "                   )    \n",
    "\n",
    "    results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\", \n",
    "                              truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    # Remove the outer list.\n",
    "    input_ids = results[0]\n",
    "\n",
    "    # Create attention masks    \n",
    "    attention_mask = [int(i>0) for i in input_ids]\n",
    "    \n",
    "    # Convert to tensors.\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    # Add an extra dimension for the \"batch\" (even though there is only one \n",
    "    # input in this batch.)\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    \n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "\n",
    " \n",
    "    # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # from all 12 layers. \n",
    "    with torch.no_grad():        \n",
    "        logits, encoded_layers = model(\n",
    "                                    input_ids = input_ids, \n",
    "                                    token_type_ids = None, \n",
    "                                    attention_mask = attention_mask,\n",
    "                                    return_dict=False)\n",
    "\n",
    "    layer_i = 12 # The last BERT layer before the classifier.\n",
    "    batch_i = 0 # Only one input in the batch.\n",
    "    token_i = 0 # The first token, corresponding to [CLS]\n",
    "        \n",
    "    # Extract the embedding.\n",
    "    embedding = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "    # Move to the CPU and convert to numpy ndarray.\n",
    "    embedding = embedding.detach().cpu().numpy()\n",
    "\n",
    "    return(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2496f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting keras\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras\n",
      "Successfully installed keras-2.9.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693a8c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (4.0.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (59.2.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (1.21.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (3.4.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (3.19.4)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.7/123.7 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.8/167.8 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 KB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.8/97.8 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.6)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.5/151.5 KB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=57719cc063e40985763013704a6ff449ac9db6ab75d5edd5b67d9851219d7437\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: termcolor, tensorboard-plugin-wit, libclang, flatbuffers, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1-modules, opt-einsum, oauthlib, keras-preprocessing, grpcio, gast, cachetools, astunparse, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.1.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.9.1 google-auth-oauthlib-0.4.6 grpcio-1.47.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9a06a",
   "metadata": {
    "papermill": {
     "duration": 0.082731,
     "end_time": "2022-01-15T06:26:12.466308",
     "exception": false,
     "start_time": "2022-01-15T06:26:12.383577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Use the model and tokenizer to generate an embedding for the 3rd input_abstract  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e131e85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T06:26:12.644191Z",
     "iopub.status.busy": "2022-01-15T06:26:12.643054Z",
     "iopub.status.idle": "2022-01-15T06:26:14.809335Z",
     "shell.execute_reply": "2022-01-15T06:26:14.810450Z",
     "shell.execute_reply.started": "2022-01-15T05:30:16.778640Z"
    },
    "papermill": {
     "duration": 2.258374,
     "end_time": "2022-01-15T06:26:14.810706",
     "exception": false,
     "start_time": "2022-01-15T06:26:12.552332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_abstract = data.abstract.iloc[3]\n",
    "\n",
    "abstract_embedding = convert_single_abstract_to_embedding(sciBERT_tokenizer, model, input_abstract)\n",
    "\n",
    "print('Embedding shape: {}'.format(abstract_embedding.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73605c5",
   "metadata": {
    "papermill": {
     "duration": 0.082601,
     "end_time": "2022-01-15T06:26:14.975353",
     "exception": false,
     "start_time": "2022-01-15T06:26:14.892752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Embedding is composed of 768 values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc7c5e",
   "metadata": {},
   "source": [
    "## Create Embedding for all the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e7cf30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T06:26:15.322865Z",
     "iopub.status.busy": "2022-01-15T06:26:15.321702Z",
     "iopub.status.idle": "2022-01-15T06:26:15.323953Z",
     "shell.execute_reply": "2022-01-15T06:26:15.324681Z",
     "shell.execute_reply.started": "2022-01-15T05:30:18.175580Z"
    },
    "papermill": {
     "duration": 0.091611,
     "end_time": "2022-01-15T06:26:15.324851",
     "exception": false,
     "start_time": "2022-01-15T06:26:15.233240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_all_abstract_text_to_embedding(df):\n",
    "    \n",
    "    # The list of all the embeddings\n",
    "    embeddings = []\n",
    "    \n",
    "    # Get overall text data\n",
    "    overall_text_data = data.abstract.values\n",
    "    \n",
    "    # Loop over all the comment and get the embeddings\n",
    "    for abstract in tqdm(overall_text_data):\n",
    "        \n",
    "        # Get the embedding \n",
    "        embedding = convert_single_abstract_to_embedding(sciBERT_tokenizer, model, abstract)\n",
    "        \n",
    "        #add it to the list\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "    print(\"Conversion Done!\")\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1df7dfe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T06:26:15.496040Z",
     "iopub.status.busy": "2022-01-15T06:26:15.494963Z",
     "iopub.status.idle": "2022-01-15T07:29:52.289239Z",
     "shell.execute_reply": "2022-01-15T07:29:52.288629Z",
     "shell.execute_reply.started": "2022-01-15T05:30:18.192680Z"
    },
    "papermill": {
     "duration": 3816.883004,
     "end_time": "2022-01-15T07:29:52.289401",
     "exception": false,
     "start_time": "2022-01-15T06:26:15.406397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3948/3948 [08:58<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This task can take a lot of time depending on the sample_size value \n",
    "\n",
    "embeddings = convert_all_abstract_text_to_embedding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0948d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings)\n",
    "np.save('embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3737f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load('embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1872e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dim = len(embeddings[0])\n",
    "embeddings_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.x = np.load('file.npy', pickle=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6429757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "import pickle\n",
    "\n",
    "# create an iterator object with write permission - embeddings.pkl\n",
    "\n",
    "with open('embeddings.pkl', 'wb') as files:\n",
    "    pickle.dump(embeddings, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dbc746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load save model \n",
    "with open('embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "617ff0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dim = len(embeddings[0])\n",
    "embeddings_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d06a4571",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T07:29:53.883703Z",
     "iopub.status.busy": "2022-01-15T07:29:53.882598Z",
     "iopub.status.idle": "2022-01-15T07:29:53.886000Z",
     "shell.execute_reply": "2022-01-15T07:29:53.885279Z",
     "shell.execute_reply.started": "2022-01-15T06:07:18.767983Z"
    },
    "papermill": {
     "duration": 0.771002,
     "end_time": "2022-01-15T07:29:53.886147",
     "exception": false,
     "start_time": "2022-01-15T07:29:53.115145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new column that will contain embedding of each body text\n",
    "def create_final_embeddings(df, embeddings):\n",
    "    \n",
    "    df[\"embeddings\"] = embeddings\n",
    "    df[\"embeddings\"] = df[\"embeddings\"].apply(lambda emb: np.array(emb))\n",
    "    df[\"embeddings\"] = df[\"embeddings\"].apply(lambda emb: emb.reshape(1, -1))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e44cb3c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T07:29:55.443836Z",
     "iopub.status.busy": "2022-01-15T07:29:55.442563Z",
     "iopub.status.idle": "2022-01-15T07:29:55.473247Z",
     "shell.execute_reply": "2022-01-15T07:29:55.473840Z",
     "shell.execute_reply.started": "2022-01-15T06:07:18.775986Z"
    },
    "papermill": {
     "duration": 0.815064,
     "end_time": "2022-01-15T07:29:55.474044",
     "exception": false,
     "start_time": "2022-01-15T07:29:54.658980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>authors</th>\n",
       "      <th>source</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The natverse: a versatile computational toolbo...</td>\n",
       "      <td>To analyse neuron data at scale, neuroscientis...</td>\n",
       "      <td>10.1101/006353</td>\n",
       "      <td>[{'author': 'Bates, A. S.', 'number on Paper':...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>[[0.75996983, -0.70971274, 0.76068234, -0.2982...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Long-range functional coupling predicts perfor...</td>\n",
       "      <td>The integration of sensory signals from differ...</td>\n",
       "      <td>10.1101/014423</td>\n",
       "      <td>[{'author': 'Wang, P.', 'number on Paper': 1, ...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>[[0.90075547, 0.055804443, 0.05721068, -0.1960...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Medial prefrontal cortex population activity i...</td>\n",
       "      <td>Cortical population activity may represent sam...</td>\n",
       "      <td>10.1101/027102</td>\n",
       "      <td>[{'author': 'Singh, A.', 'number on Paper': 1,...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>[[-0.11369413, -0.3029109, 0.44304308, -0.6325...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  The natverse: a versatile computational toolbo...   \n",
       "1  Long-range functional coupling predicts perfor...   \n",
       "2  Medial prefrontal cortex population activity i...   \n",
       "\n",
       "                                            abstract             doi  \\\n",
       "0  To analyse neuron data at scale, neuroscientis...  10.1101/006353   \n",
       "1  The integration of sensory signals from differ...  10.1101/014423   \n",
       "2  Cortical population activity may represent sam...  10.1101/027102   \n",
       "\n",
       "                                             authors    source  \\\n",
       "0  [{'author': 'Bates, A. S.', 'number on Paper':...  bioarxiv   \n",
       "1  [{'author': 'Wang, P.', 'number on Paper': 1, ...  bioarxiv   \n",
       "2  [{'author': 'Singh, A.', 'number on Paper': 1,...  bioarxiv   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [[0.75996983, -0.70971274, 0.76068234, -0.2982...  \n",
       "1  [[0.90075547, 0.055804443, 0.05721068, -0.1960...  \n",
       "2  [[-0.11369413, -0.3029109, 0.44304308, -0.6325...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = create_final_embeddings(data, embeddings)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c975c",
   "metadata": {
    "papermill": {
     "duration": 0.768636,
     "end_time": "2022-01-15T07:29:57.018467",
     "exception": false,
     "start_time": "2022-01-15T07:29:56.249831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cosine Similarity Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd3c0c",
   "metadata": {
    "papermill": {
     "duration": 0.770171,
     "end_time": "2022-01-15T07:29:58.559383",
     "exception": false,
     "start_time": "2022-01-15T07:29:57.789212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ff2309d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T07:30:00.430667Z",
     "iopub.status.busy": "2022-01-15T07:30:00.429547Z",
     "iopub.status.idle": "2022-01-15T07:30:00.431795Z",
     "shell.execute_reply": "2022-01-15T07:30:00.432938Z",
     "shell.execute_reply.started": "2022-01-15T06:07:18.821141Z"
    },
    "papermill": {
     "duration": 1.096778,
     "end_time": "2022-01-15T07:30:00.433107",
     "exception": false,
     "start_time": "2022-01-15T07:29:59.336329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_query(query_text):\n",
    "    \"\"\" \n",
    "    Create a vector for given query and adjust it for cosine similarity search\n",
    "    \"\"\"\n",
    "\n",
    "    query_vect = convert_single_abstract_to_embedding(sciBERT_tokenizer, model, query_text)\n",
    "    query_vect = np.array(query_vect)\n",
    "    query_vect = query_vect.reshape(1, -1)\n",
    "    return query_vect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf93ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_N_similar_articles_cosine(query_text, data, top_N=10):\n",
    "    \"\"\"\n",
    "    Retrieve top_N (10 is default value) article abstract similar to the query\n",
    "    \"\"\"\n",
    "    query_vect = process_query(query_text)\n",
    "    revevant_cols = [\"title\", \"abstract\",  \"doi\", \"authors\", \"source\", \"cosine_similarity\"]\n",
    "    \n",
    "    # Run similarity Search\n",
    "    data[\"cosine_similarity\"] = data[\"embeddings\"].apply(lambda x: cosine_similarity(query_vect, x))\n",
    "    data[\"cosine_similarity\"] = data[\"cosine_similarity\"].apply(lambda x: x[0][0])\n",
    "    \n",
    "    \"\"\"\n",
    "    Sort Cosine Similarity Column in Descending Order.\n",
    "    Below index starts at 1 to remove similarity with itself because it is always 1.\n",
    "    \"\"\"\n",
    "    most_similar_articles = data.sort_values(by='cosine_similarity', ascending=False)[1:top_N+1]\n",
    "    \n",
    "    return most_similar_articles[revevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c9f1105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T07:30:03.635859Z",
     "iopub.status.busy": "2022-01-15T07:30:03.634295Z",
     "iopub.status.idle": "2022-01-15T07:30:06.311902Z",
     "shell.execute_reply": "2022-01-15T07:30:06.311320Z",
     "shell.execute_reply.started": "2022-01-15T06:07:18.830702Z"
    },
    "papermill": {
     "duration": 3.466171,
     "end_time": "2022-01-15T07:30:06.312068",
     "exception": false,
     "start_time": "2022-01-15T07:30:02.845897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_text_test = data.iloc[0].abstract # query abstract input\n",
    "\n",
    "top_articles = get_top_N_similar_articles_cosine(query_text_test, data) # 10 similar recommendations in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "647a2d77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T07:30:07.843342Z",
     "iopub.status.busy": "2022-01-15T07:30:07.842412Z",
     "iopub.status.idle": "2022-01-15T07:30:07.846454Z",
     "shell.execute_reply": "2022-01-15T07:30:07.847192Z",
     "shell.execute_reply.started": "2022-01-15T06:07:20.412576Z"
    },
    "papermill": {
     "duration": 0.777271,
     "end_time": "2022-01-15T07:30:07.847361",
     "exception": false,
     "start_time": "2022-01-15T07:30:07.070090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>authors</th>\n",
       "      <th>source</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>Open Source Brain: a collaborative resource fo...</td>\n",
       "      <td>Computational models are powerful tools for in...</td>\n",
       "      <td>10.1101/229484</td>\n",
       "      <td>[{'author': 'Gleeson, P.', 'number on Paper': ...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.897324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>AFQ-Browser: Supporting reproducible human neu...</td>\n",
       "      <td>Human neuroscience research faces several chal...</td>\n",
       "      <td>10.1101/182402</td>\n",
       "      <td>[{'author': 'Yeatman, J. D.', 'number on Paper...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.895607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>AFQ-Browser: Supporting reproducible human neu...</td>\n",
       "      <td>Human neuroscience research faces several chal...</td>\n",
       "      <td>10.1101/182402</td>\n",
       "      <td>[{'author': 'Yeatman, J. D.', 'number on Paper...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.895607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>Real-time experimental control using network-b...</td>\n",
       "      <td>Modern neuroscience research often requires th...</td>\n",
       "      <td>10.1101/392654</td>\n",
       "      <td>[{'author': 'Kim, B.', 'number on Paper': 1, '...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.894662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>A low-cost hyperspectral scanner for natural i...</td>\n",
       "      <td>Hyperspectral imaging is a widely used technol...</td>\n",
       "      <td>10.1101/322172</td>\n",
       "      <td>[{'author': 'Nevala, N. E.', 'number on Paper'...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.892926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>AutonoMouse: High throughput automated operant...</td>\n",
       "      <td>Operant conditioning is a crucial tool in neur...</td>\n",
       "      <td>10.1101/291815</td>\n",
       "      <td>[{'author': 'Erskine, A.', 'number on Paper': ...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.885081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>Regional protein expression in human Alzheimer...</td>\n",
       "      <td>Alzheimers disease (AD) is a progressive neuro...</td>\n",
       "      <td>10.1101/283705</td>\n",
       "      <td>[{'author': 'Xu, J.', 'number on Paper': 1, 'i...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.884527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>Variation among intact tissue samples reveals ...</td>\n",
       "      <td>It is widely assumed that cells must be physic...</td>\n",
       "      <td>10.1101/265397</td>\n",
       "      <td>[{'author': 'Kelley, K. W.', 'number on Paper'...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.877315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>An Open-source Tool for Analysis and Automatic...</td>\n",
       "      <td>Synaptic plasticity, the cellular basis for le...</td>\n",
       "      <td>10.1101/281667</td>\n",
       "      <td>[{'author': 'Smirnov, M. S.', 'number on Paper...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.874579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>Inferring and validating mechanistic models of...</td>\n",
       "      <td>The interpretation of neuronal spike train rec...</td>\n",
       "      <td>10.1101/261016</td>\n",
       "      <td>[{'author': 'Ladenbauer, J.', 'number on Paper...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.874049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "779   Open Source Brain: a collaborative resource fo...   \n",
       "282   AFQ-Browser: Supporting reproducible human neu...   \n",
       "281   AFQ-Browser: Supporting reproducible human neu...   \n",
       "3933  Real-time experimental control using network-b...   \n",
       "2440  A low-cost hyperspectral scanner for natural i...   \n",
       "1801  AutonoMouse: High throughput automated operant...   \n",
       "1622  Regional protein expression in human Alzheimer...   \n",
       "1203  Variation among intact tissue samples reveals ...   \n",
       "1591  An Open-source Tool for Analysis and Automatic...   \n",
       "1133  Inferring and validating mechanistic models of...   \n",
       "\n",
       "                                               abstract             doi  \\\n",
       "779   Computational models are powerful tools for in...  10.1101/229484   \n",
       "282   Human neuroscience research faces several chal...  10.1101/182402   \n",
       "281   Human neuroscience research faces several chal...  10.1101/182402   \n",
       "3933  Modern neuroscience research often requires th...  10.1101/392654   \n",
       "2440  Hyperspectral imaging is a widely used technol...  10.1101/322172   \n",
       "1801  Operant conditioning is a crucial tool in neur...  10.1101/291815   \n",
       "1622  Alzheimers disease (AD) is a progressive neuro...  10.1101/283705   \n",
       "1203  It is widely assumed that cells must be physic...  10.1101/265397   \n",
       "1591  Synaptic plasticity, the cellular basis for le...  10.1101/281667   \n",
       "1133  The interpretation of neuronal spike train rec...  10.1101/261016   \n",
       "\n",
       "                                                authors    source  \\\n",
       "779   [{'author': 'Gleeson, P.', 'number on Paper': ...  bioarxiv   \n",
       "282   [{'author': 'Yeatman, J. D.', 'number on Paper...  bioarxiv   \n",
       "281   [{'author': 'Yeatman, J. D.', 'number on Paper...  bioarxiv   \n",
       "3933  [{'author': 'Kim, B.', 'number on Paper': 1, '...  bioarxiv   \n",
       "2440  [{'author': 'Nevala, N. E.', 'number on Paper'...  bioarxiv   \n",
       "1801  [{'author': 'Erskine, A.', 'number on Paper': ...  bioarxiv   \n",
       "1622  [{'author': 'Xu, J.', 'number on Paper': 1, 'i...  bioarxiv   \n",
       "1203  [{'author': 'Kelley, K. W.', 'number on Paper'...  bioarxiv   \n",
       "1591  [{'author': 'Smirnov, M. S.', 'number on Paper...  bioarxiv   \n",
       "1133  [{'author': 'Ladenbauer, J.', 'number on Paper...  bioarxiv   \n",
       "\n",
       "      cosine_similarity  \n",
       "779            0.897324  \n",
       "282            0.895607  \n",
       "281            0.895607  \n",
       "3933           0.894662  \n",
       "2440           0.892926  \n",
       "1801           0.885081  \n",
       "1622           0.884527  \n",
       "1203           0.877315  \n",
       "1591           0.874579  \n",
       "1133           0.874049  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8251e170",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T07:30:09.388046Z",
     "iopub.status.busy": "2022-01-15T07:30:09.387274Z",
     "iopub.status.idle": "2022-01-15T07:30:09.391102Z",
     "shell.execute_reply": "2022-01-15T07:30:09.391650Z",
     "shell.execute_reply.started": "2022-01-15T06:07:20.425161Z"
    },
    "papermill": {
     "duration": 0.778359,
     "end_time": "2022-01-15T07:30:09.391809",
     "exception": false,
     "start_time": "2022-01-15T07:30:08.613450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Computational models are powerful tools for investigating brain function in health and disease. However, biologically detailed neuronal and circuit models are complex and implemented in a range of specialized languages, making them inaccessible and opaque to many neuroscientists. This has limited critical evaluation of models by the scientific community and impeded their refinement and widespread adoption. To address this, we have combined advances in standardizing models, open source software development and web technologies to develop Open Source Brain, a platform for visualizing, simulating, disseminating and collaboratively developing standardized models of neurons and circuits from a range of brain regions. Model structure and parameters can be visualized and their dynamical properties explored through browser-controlled simulations, without writing code. Open Source Brain makes neural models transparent and accessible and facilitates testing, critical evaluation and refinement, thereby helping to improve the accuracy and reproducibility of models, and their dissemination to the wider community.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_articles.iloc[0].abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8c08d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'author': 'Gleeson, P.',\n",
       "  'number on Paper': 1,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Cantarelli, M.',\n",
       "  'number on Paper': 2,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Marin, B.',\n",
       "  'number on Paper': 3,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Quintana, A.',\n",
       "  'number on Paper': 4,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Earnshaw, M.',\n",
       "  'number on Paper': 5,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Piasini, E.',\n",
       "  'number on Paper': 6,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Birgiolas, J.',\n",
       "  'number on Paper': 7,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Cannon, R. C.',\n",
       "  'number on Paper': 8,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Cayco-Gajic, N. A.',\n",
       "  'number on Paper': 9,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Crook, S.',\n",
       "  'number on Paper': 10,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Davison, A. P.',\n",
       "  'number on Paper': 11,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Dura-Bernal, S.',\n",
       "  'number on Paper': 12,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Ecker, A.',\n",
       "  'number on Paper': 13,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Hines, M. L.',\n",
       "  'number on Paper': 14,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Idili, G.',\n",
       "  'number on Paper': 15,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Larson, S.',\n",
       "  'number on Paper': 16,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Lytton, W. W.',\n",
       "  'number on Paper': 17,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Majumdar, A.',\n",
       "  'number on Paper': 18,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' McDougal, R. A.',\n",
       "  'number on Paper': 19,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Sivagnanam, S.',\n",
       "  'number on Paper': 20,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Solinas, S.',\n",
       "  'number on Paper': 21,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Stanislovas, R.',\n",
       "  'number on Paper': 22,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' van Albada, S. J.',\n",
       "  'number on Paper': 23,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Van Geit, W.',\n",
       "  'number on Paper': 24,\n",
       "  'institution': 'University College London'},\n",
       " {'author': ' Silver, R. A.',\n",
       "  'number on Paper': 25,\n",
       "  'institution': 'University College London'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_articles.iloc[0].authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98b427d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T07:30:11.239395Z",
     "iopub.status.busy": "2022-01-15T07:30:11.237842Z",
     "iopub.status.idle": "2022-01-15T07:30:11.241828Z",
     "shell.execute_reply": "2022-01-15T07:30:11.242394Z",
     "shell.execute_reply.started": "2022-01-15T06:07:20.436320Z"
    },
    "papermill": {
     "duration": 1.084305,
     "end_time": "2022-01-15T07:30:11.242582",
     "exception": false,
     "start_time": "2022-01-15T07:30:10.158277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human neuroscience research faces several challenges with regards to reproducibility. While scientists are generally aware that data sharing is an important component of reproducible research, it is not always clear how to usefully share data in a manner that allows other labs to understand and reproduce published findings. Here we describe a new open source tool, AFQ-Browser, that builds an interactive website as a companion to a published diffusion MRI study. Because AFQ-browser is portable -- it runs in any modern web-browser -- it can facilitate transparency and data sharing. Moreover, by leveraging new web-visualization technologies to create linked views between different dimensions of a diffusion MRI dataset (anatomy, quantitative diffusion metrics, subject metadata), AFQ-Browser facilitates exploratory data analysis, fueling new scientific discoveries based on previously published datasets. In an era where Big Data is playing an increasingly prominent role in scientific discovery, so will browser-based tools for exploring high-dimensional datasets, communicating scientific discoveries, sharing and aggregating data across labs, and publishing data alongside manuscripts.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_articles.iloc[1].abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1df07f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T07:30:12.787304Z",
     "iopub.status.busy": "2022-01-15T07:30:12.786384Z",
     "iopub.status.idle": "2022-01-15T07:30:12.790203Z",
     "shell.execute_reply": "2022-01-15T07:30:12.790772Z",
     "shell.execute_reply.started": "2022-01-15T06:07:20.447286Z"
    },
    "papermill": {
     "duration": 0.776279,
     "end_time": "2022-01-15T07:30:12.790965",
     "exception": false,
     "start_time": "2022-01-15T07:30:12.014686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human neuroscience research faces several challenges with regards to reproducibility. While scientists are generally aware that data sharing is an important component of reproducible research, it is not always clear how to usefully share data in a manner that allows other labs to understand and reproduce published findings. Here we describe a new open source tool, AFQ-Browser, that builds an interactive website as a companion to a published diffusion MRI study. Because AFQ-browser is portable -- it runs in any modern web-browser -- it can facilitate transparency and data sharing. Moreover, by leveraging new web-visualization technologies to create linked views between different dimensions of a diffusion MRI dataset (anatomy, quantitative diffusion metrics, subject metadata), AFQ-Browser facilitates exploratory data analysis, fueling new scientific discoveries based on previously published datasets. In an era where Big Data is playing an increasingly prominent role in scientific discovery, so will browser-based tools for exploring high-dimensional datasets, communicating scientific discoveries, sharing and aggregating data across labs, and publishing data alongside manuscripts.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_articles.iloc[2].abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a71de9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T07:30:14.322396Z",
     "iopub.status.busy": "2022-01-15T07:30:14.321387Z",
     "iopub.status.idle": "2022-01-15T07:30:14.325267Z",
     "shell.execute_reply": "2022-01-15T07:30:14.325803Z",
     "shell.execute_reply.started": "2022-01-15T06:07:20.458803Z"
    },
    "papermill": {
     "duration": 0.777504,
     "end_time": "2022-01-15T07:30:14.325988",
     "exception": false,
     "start_time": "2022-01-15T07:30:13.548484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modern neuroscience research often requires the coordination of multiple processes such as stimulus generation, real-time experimental control, as well as behavioral and neural measurements. The technical demands required to simultaneously manage these processes with high temporal fidelity limits the number of labs capable of performing such work. Here we present an open-source network-based parallel processing framework that eliminates these barriers. The Real-Time Experimental Control with Graphical User Interface (REC-GUI) framework offers multiple advantages: (i) a modular design agnostic to coding language(s) and operating system(s) that maximizes experimental flexibility and minimizes researcher effort, (ii) simple interfacing to connect measurement and recording devices, (iii) high temporal fidelity by dividing task demands across CPUs, and (iv) real-time control using a fully customizable and intuitive GUI. Testing results demonstrate that the REC-GUI framework facilitates technically demanding, behavior-contingent neuroscience research. Sample code and hardware configurations are downloadable, and future developments will be regularly released.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_articles.iloc[3].abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0088e2",
   "metadata": {},
   "source": [
    "# Cosine Similarity for user input abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "247d6e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our visual environment impacts multiple aspects of cognition including perception, attention and memory, yet most studies traditionally remove or control the external environment. As a result, we have a limited understanding of neurocognitive processes beyond the controlled lab environment. Here, we aim to study neural processes in real-world environments, while also maintaining a degree of control over perception. To achieve this, we combined mobile EEG (mEEG) and augmented reality (AR), which allows us to place virtual objects into the real world. We validated this AR and mEEG approach using a well-characterised cognitive response-the face inversion effect. Participants viewed upright and inverted faces in three EEG tasks (1) a lab-based computer task, (2) walking through an indoor environment while seeing face photographs, and (3) walking through an indoor environment while seeing virtual faces. We find greater low frequency EEG activity for inverted compared to upright faces in all experimental tasks, demonstrating that cognitively relevant signals can be extracted from mEEG and AR paradigms. This was established in both an epoch-based analysis aligned to face events, and a GLM-based approach that incorporates continuous EEG signals and face perception states. Together, this research helps pave the way to exploring neurocognitive processes in real-world environments while maintaining experimental control using AR.\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "********************     RECOMMENDATIONS     *************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_text_test = str(input())\n",
    "print(\"--------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "print(\"********************     RECOMMENDATIONS     *************\\n\")\n",
    "\n",
    "top_articles = get_top_N_similar_articles_cosine(query_text_test, data)  # take input from user and recommend top 10 using cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba93219f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>authors</th>\n",
       "      <th>source</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3102</th>\n",
       "      <td>Pins &amp; Needles: Towards Limb Disownership in A...</td>\n",
       "      <td>The seemingly stable construct of our bodily s...</td>\n",
       "      <td>10.1101/349795</td>\n",
       "      <td>[{'author': 'Kannape, O. A.', 'number on Paper...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.915672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>Decoding digits and dice with Magnetoencephalo...</td>\n",
       "      <td>Numerical format describes the way magnitude i...</td>\n",
       "      <td>10.1101/249342</td>\n",
       "      <td>[{'author': 'Teichmann, L.', 'number on Paper'...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.913674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>word2brain</td>\n",
       "      <td>Mapping brain functions to their underlying ne...</td>\n",
       "      <td>10.1101/299024</td>\n",
       "      <td>[{'author': 'Nunes, A.', 'number on Paper': 1,...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.912345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>More is Better: Using Machine Learning Techniq...</td>\n",
       "      <td>A basic aim of marketing research is to predic...</td>\n",
       "      <td>10.1101/317073</td>\n",
       "      <td>[{'author': 'Hakim, A.', 'number on Paper': 1,...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.911026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>Pathways to Consumers Minds: Using Machine Lea...</td>\n",
       "      <td>A basic aim of marketing research is to predic...</td>\n",
       "      <td>10.1101/317073</td>\n",
       "      <td>[{'author': 'Hakim, A.', 'number on Paper': 1,...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.911026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>Real-time decoding of selective attention from...</td>\n",
       "      <td>Humans are highly skilled at analysing complex...</td>\n",
       "      <td>10.1101/259853</td>\n",
       "      <td>[{'author': 'Etard, O.', 'number on Paper': 1,...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.910582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>Decoding of selective attention to continuous ...</td>\n",
       "      <td>Humans are highly skilled at analysing complex...</td>\n",
       "      <td>10.1101/259853</td>\n",
       "      <td>[{'author': 'Etard, O.', 'number on Paper': 1,...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.910582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>Alpha-band oscillations track the retrieval of...</td>\n",
       "      <td>A hallmark of episodic memory is the phenomeno...</td>\n",
       "      <td>10.1101/207860</td>\n",
       "      <td>[{'author': 'Sutterer, D. W.', 'number on Pape...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.909132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152</th>\n",
       "      <td>Non-assortative community structure in resting...</td>\n",
       "      <td>Brain networks exhibit community structure tha...</td>\n",
       "      <td>10.1101/355016</td>\n",
       "      <td>[{'author': 'Betzel, R. F.', 'number on Paper'...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.908134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>Spotting the path that leads nowhere: Modulati...</td>\n",
       "      <td>The capacity to take efficient detours and exp...</td>\n",
       "      <td>10.1101/301697</td>\n",
       "      <td>[{'author': 'Javadi, A.-H.', 'number on Paper'...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.907643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "3102  Pins & Needles: Towards Limb Disownership in A...   \n",
       "906   Decoding digits and dice with Magnetoencephalo...   \n",
       "1984                                         word2brain   \n",
       "2343  More is Better: Using Machine Learning Techniq...   \n",
       "2344  Pathways to Consumers Minds: Using Machine Lea...   \n",
       "1081  Real-time decoding of selective attention from...   \n",
       "1082  Decoding of selective attention to continuous ...   \n",
       "1232  Alpha-band oscillations track the retrieval of...   \n",
       "3152  Non-assortative community structure in resting...   \n",
       "2038  Spotting the path that leads nowhere: Modulati...   \n",
       "\n",
       "                                               abstract             doi  \\\n",
       "3102  The seemingly stable construct of our bodily s...  10.1101/349795   \n",
       "906   Numerical format describes the way magnitude i...  10.1101/249342   \n",
       "1984  Mapping brain functions to their underlying ne...  10.1101/299024   \n",
       "2343  A basic aim of marketing research is to predic...  10.1101/317073   \n",
       "2344  A basic aim of marketing research is to predic...  10.1101/317073   \n",
       "1081  Humans are highly skilled at analysing complex...  10.1101/259853   \n",
       "1082  Humans are highly skilled at analysing complex...  10.1101/259853   \n",
       "1232  A hallmark of episodic memory is the phenomeno...  10.1101/207860   \n",
       "3152  Brain networks exhibit community structure tha...  10.1101/355016   \n",
       "2038  The capacity to take efficient detours and exp...  10.1101/301697   \n",
       "\n",
       "                                                authors    source  \\\n",
       "3102  [{'author': 'Kannape, O. A.', 'number on Paper...  bioarxiv   \n",
       "906   [{'author': 'Teichmann, L.', 'number on Paper'...  bioarxiv   \n",
       "1984  [{'author': 'Nunes, A.', 'number on Paper': 1,...  bioarxiv   \n",
       "2343  [{'author': 'Hakim, A.', 'number on Paper': 1,...  bioarxiv   \n",
       "2344  [{'author': 'Hakim, A.', 'number on Paper': 1,...  bioarxiv   \n",
       "1081  [{'author': 'Etard, O.', 'number on Paper': 1,...  bioarxiv   \n",
       "1082  [{'author': 'Etard, O.', 'number on Paper': 1,...  bioarxiv   \n",
       "1232  [{'author': 'Sutterer, D. W.', 'number on Pape...  bioarxiv   \n",
       "3152  [{'author': 'Betzel, R. F.', 'number on Paper'...  bioarxiv   \n",
       "2038  [{'author': 'Javadi, A.-H.', 'number on Paper'...  bioarxiv   \n",
       "\n",
       "      cosine_similarity  \n",
       "3102           0.915672  \n",
       "906            0.913674  \n",
       "1984           0.912345  \n",
       "2343           0.911026  \n",
       "2344           0.911026  \n",
       "1081           0.910582  \n",
       "1082           0.910582  \n",
       "1232           0.909132  \n",
       "3152           0.908134  \n",
       "2038           0.907643  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_articles # top 10 recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b677d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_top_10 = top_articles.to_json('cos_sim_top_10.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8529728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (10, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>authors</th>\n",
       "      <th>source</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3102</th>\n",
       "      <td>Pins &amp; Needles: Towards Limb Disownership in A...</td>\n",
       "      <td>The seemingly stable construct of our bodily s...</td>\n",
       "      <td>10.1101/349795</td>\n",
       "      <td>[{'author': 'Kannape, O. A.', 'number on Paper...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.915672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>Decoding digits and dice with Magnetoencephalo...</td>\n",
       "      <td>Numerical format describes the way magnitude i...</td>\n",
       "      <td>10.1101/249342</td>\n",
       "      <td>[{'author': 'Teichmann, L.', 'number on Paper'...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.913674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>word2brain</td>\n",
       "      <td>Mapping brain functions to their underlying ne...</td>\n",
       "      <td>10.1101/299024</td>\n",
       "      <td>[{'author': 'Nunes, A.', 'number on Paper': 1,...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.912345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>More is Better: Using Machine Learning Techniq...</td>\n",
       "      <td>A basic aim of marketing research is to predic...</td>\n",
       "      <td>10.1101/317073</td>\n",
       "      <td>[{'author': 'Hakim, A.', 'number on Paper': 1,...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.911026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>Pathways to Consumers Minds: Using Machine Lea...</td>\n",
       "      <td>A basic aim of marketing research is to predic...</td>\n",
       "      <td>10.1101/317073</td>\n",
       "      <td>[{'author': 'Hakim, A.', 'number on Paper': 1,...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.911026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>Real-time decoding of selective attention from...</td>\n",
       "      <td>Humans are highly skilled at analysing complex...</td>\n",
       "      <td>10.1101/259853</td>\n",
       "      <td>[{'author': 'Etard, O.', 'number on Paper': 1,...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.910582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>Decoding of selective attention to continuous ...</td>\n",
       "      <td>Humans are highly skilled at analysing complex...</td>\n",
       "      <td>10.1101/259853</td>\n",
       "      <td>[{'author': 'Etard, O.', 'number on Paper': 1,...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.910582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>Alpha-band oscillations track the retrieval of...</td>\n",
       "      <td>A hallmark of episodic memory is the phenomeno...</td>\n",
       "      <td>10.1101/207860</td>\n",
       "      <td>[{'author': 'Sutterer, D. W.', 'number on Pape...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.909132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152</th>\n",
       "      <td>Non-assortative community structure in resting...</td>\n",
       "      <td>Brain networks exhibit community structure tha...</td>\n",
       "      <td>10.1101/355016</td>\n",
       "      <td>[{'author': 'Betzel, R. F.', 'number on Paper'...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.908134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>Spotting the path that leads nowhere: Modulati...</td>\n",
       "      <td>The capacity to take efficient detours and exp...</td>\n",
       "      <td>10.1101/301697</td>\n",
       "      <td>[{'author': 'Javadi, A.-H.', 'number on Paper'...</td>\n",
       "      <td>bioarxiv</td>\n",
       "      <td>0.907643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "3102  Pins & Needles: Towards Limb Disownership in A...   \n",
       "906   Decoding digits and dice with Magnetoencephalo...   \n",
       "1984                                         word2brain   \n",
       "2343  More is Better: Using Machine Learning Techniq...   \n",
       "2344  Pathways to Consumers Minds: Using Machine Lea...   \n",
       "1081  Real-time decoding of selective attention from...   \n",
       "1082  Decoding of selective attention to continuous ...   \n",
       "1232  Alpha-band oscillations track the retrieval of...   \n",
       "3152  Non-assortative community structure in resting...   \n",
       "2038  Spotting the path that leads nowhere: Modulati...   \n",
       "\n",
       "                                               abstract             doi  \\\n",
       "3102  The seemingly stable construct of our bodily s...  10.1101/349795   \n",
       "906   Numerical format describes the way magnitude i...  10.1101/249342   \n",
       "1984  Mapping brain functions to their underlying ne...  10.1101/299024   \n",
       "2343  A basic aim of marketing research is to predic...  10.1101/317073   \n",
       "2344  A basic aim of marketing research is to predic...  10.1101/317073   \n",
       "1081  Humans are highly skilled at analysing complex...  10.1101/259853   \n",
       "1082  Humans are highly skilled at analysing complex...  10.1101/259853   \n",
       "1232  A hallmark of episodic memory is the phenomeno...  10.1101/207860   \n",
       "3152  Brain networks exhibit community structure tha...  10.1101/355016   \n",
       "2038  The capacity to take efficient detours and exp...  10.1101/301697   \n",
       "\n",
       "                                                authors    source  \\\n",
       "3102  [{'author': 'Kannape, O. A.', 'number on Paper...  bioarxiv   \n",
       "906   [{'author': 'Teichmann, L.', 'number on Paper'...  bioarxiv   \n",
       "1984  [{'author': 'Nunes, A.', 'number on Paper': 1,...  bioarxiv   \n",
       "2343  [{'author': 'Hakim, A.', 'number on Paper': 1,...  bioarxiv   \n",
       "2344  [{'author': 'Hakim, A.', 'number on Paper': 1,...  bioarxiv   \n",
       "1081  [{'author': 'Etard, O.', 'number on Paper': 1,...  bioarxiv   \n",
       "1082  [{'author': 'Etard, O.', 'number on Paper': 1,...  bioarxiv   \n",
       "1232  [{'author': 'Sutterer, D. W.', 'number on Pape...  bioarxiv   \n",
       "3152  [{'author': 'Betzel, R. F.', 'number on Paper'...  bioarxiv   \n",
       "2038  [{'author': 'Javadi, A.-H.', 'number on Paper'...  bioarxiv   \n",
       "\n",
       "      cosine_similarity  \n",
       "3102           0.915672  \n",
       "906            0.913674  \n",
       "1984           0.912345  \n",
       "2343           0.911026  \n",
       "2344           0.911026  \n",
       "1081           0.910582  \n",
       "1082           0.910582  \n",
       "1232           0.909132  \n",
       "3152           0.908134  \n",
       "2038           0.907643  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_sim = pd.read_json(\"cos_sim_top_10.json\") \n",
    "print(\"Data Shape: {}\".format(df_cos_sim.shape))\n",
    "df_cos_sim.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "725d7e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The seemingly stable construct of our bodily self depends on the continued, successful integration of multisensory feedback about our body, rather than its purely physical composition. Accordingly, pathological disruption of such neural processing is linked to striking alterations of the bodily self, ranging from limb misidentification to disownership, and even the desire to amputate a healthy limb. While previous embodiment research has relied on experimental setups using supernumerary limbs in variants of the Rubber Hand Illusion, we here used Augmented Reality to directly manipulate the feeling of ownership for ones own, biological limb. Using a Head-Mounted Display, participants received visual feedback about their own arm, from an embodied first-person perspective. In a series of three studies, in independent cohorts, we altered embodiment by providing visuotactile feedback that could be synchronous (control condition) or asynchronous (400ms delay, Real Hand Illusion). During the illusion, participants reported a significant decrease in ownership of their own limb, along with a lowered sense of agency. Supporting the right-parietal body network, we found an increased illusion strength for the left upper limb as well as a modulation of the feeling of ownership during anodal transcranial direct current stimulation. Extending previous research, these findings demonstrate that a controlled, visuotactile conflict about ones own limb can be used to directly and systematically modulate ownership - without a proxy. This not only corroborates the malleability of body representation but questions its permanence. These findings warrant further exploration of combined VR and neuromodulation therapies for disorders of the bodily self.'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_articles.iloc[0].abstract  # read abstract to see research similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8136b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'author': 'Kannape, O. A.',\n",
       "  'number on Paper': 1,\n",
       "  'institution': 'Swiss Institute of Technology Lausanne (EPFL)'},\n",
       " {'author': ' Smith, E. J.',\n",
       "  'number on Paper': 2,\n",
       "  'institution': 'Swiss Institute of Technology Lausanne (EPFL)'},\n",
       " {'author': ' Moseley, P.',\n",
       "  'number on Paper': 3,\n",
       "  'institution': 'Swiss Institute of Technology Lausanne (EPFL)'},\n",
       " {'author': ' Roy, M.',\n",
       "  'number on Paper': 4,\n",
       "  'institution': 'Swiss Institute of Technology Lausanne (EPFL)'},\n",
       " {'author': ' Lenggenhager, B.',\n",
       "  'number on Paper': 5,\n",
       "  'institution': 'Swiss Institute of Technology Lausanne (EPFL)'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_articles.iloc[0].authors  # top recommended reviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82b51525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'author': 'Nunes, A.',\n",
       "  'number on Paper': 1,\n",
       "  'institution': 'Dalhousie University'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_articles.iloc[2].authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e0ae7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3102    [{'author': 'Kannape, O. A.', 'number on Paper...\n",
       "906     [{'author': 'Teichmann, L.', 'number on Paper'...\n",
       "1984    [{'author': 'Nunes, A.', 'number on Paper': 1,...\n",
       "2343    [{'author': 'Hakim, A.', 'number on Paper': 1,...\n",
       "2344    [{'author': 'Hakim, A.', 'number on Paper': 1,...\n",
       "1081    [{'author': 'Etard, O.', 'number on Paper': 1,...\n",
       "1082    [{'author': 'Etard, O.', 'number on Paper': 1,...\n",
       "1232    [{'author': 'Sutterer, D. W.', 'number on Pape...\n",
       "3152    [{'author': 'Betzel, R. F.', 'number on Paper'...\n",
       "2038    [{'author': 'Javadi, A.-H.', 'number on Paper'...\n",
       "Name: authors, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_articles.authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725651e2",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "1. @inproceedings{beltagy-etal-2019-scibert,\n",
    "    title = \"SciBERT: A Pretrained Language Model for Scientific Text\",\n",
    "    author = \"Beltagy, Iz  and Lo, Kyle  and Cohan, Arman\",\n",
    "    booktitle = \"EMNLP\",\n",
    "    year = \"2019\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://www.aclweb.org/anthology/D19-1371\"\n",
    "}\n",
    "\n",
    "2. @article{johnson2019billion,\n",
    "  title={Billion-scale similarity search with {GPUs}},\n",
    "  author={Johnson, Jeff and Douze, Matthijs and J{\\'e}gou, Herv{\\'e}},\n",
    "  journal={IEEE Transactions on Big Data},\n",
    "  volume={7},\n",
    "  number={3},\n",
    "  pages={535--547},\n",
    "  year={2019},\n",
    "  publisher={IEEE}\n",
    "}\n",
    "\n",
    "3. “Bert Word Embeddings Tutorial.” BERT Word Embeddings Tutorial · Chris McCormick, 14 May 2019, https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#3-extracting-embeddings. \n",
    "\n",
    "4. Keita, Zoumana. “Scientific Documents Similarity Search with Deep Learning Using Transformers (Scibert).” Medium, Towards Data Science, 17 Jan. 2022, https://towardsdatascience.com/scientific-documents-similarity-search-with-deep-learning-using-transformers-scibert-d47c4e501590. \n",
    "\n",
    "5. @article{Beltagy2020Longformer,\n",
    "  title={Longformer: The Long-Document Transformer},\n",
    "  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},\n",
    "  journal={arXiv:2004.05150},\n",
    "  year={2020},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37170573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3968.222771,
   "end_time": "2022-01-15T07:30:41.940272",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-15T06:24:33.717501",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00e8f51bfab94006a174348bfa299c7b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "091678bec26a44e1aa2c0e4faeb117f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "09225643d35b4512a0812f87855b76c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cccbfcc8a6a64691ad4a1667504388d2",
       "placeholder": "​",
       "style": "IPY_MODEL_3cf96a463ecf4f659544e847d66979ad",
       "value": " 385/385 [00:00&lt;00:00, 11.0kB/s]"
      }
     },
     "26b1c416648e4e6a827a603be146946b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_365e20c488724d8ea1c5c2199f78bd76",
       "max": 385,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_091678bec26a44e1aa2c0e4faeb117f8",
       "value": 385
      }
     },
     "290bf1ac0031410ba295792e01560623": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ec461606d3114ca79862034c9806c968",
       "placeholder": "​",
       "style": "IPY_MODEL_d206eeb3d5a547b9b2a2d18b7eabbc52",
       "value": "Downloading: 100%"
      }
     },
     "365e20c488724d8ea1c5c2199f78bd76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3766b78fb5e44941a1c5c126538a800d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b4b1b1bac634c0da3484a7ab9526c99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e4e933581e9d495c927cd6d4737ad59d",
        "IPY_MODEL_26b1c416648e4e6a827a603be146946b",
        "IPY_MODEL_09225643d35b4512a0812f87855b76c2"
       ],
       "layout": "IPY_MODEL_50a70c29e6bf4c3d81b3bf4a42232ac4"
      }
     },
     "3cf96a463ecf4f659544e847d66979ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4169d7d17ef74e17809015539122e940": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4aadedc0f7ff4f0c846ec784e712f3da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ba8aa29701b8461e916f561274781924",
       "max": 442221694,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_62287e4e175a420a8b496c41509f0a78",
       "value": 442221694
      }
     },
     "50a70c29e6bf4c3d81b3bf4a42232ac4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "62287e4e175a420a8b496c41509f0a78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "77de0c4ca0f14bd0a533615ef53cbd83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c15a120aec2549b491814d4e37b303c7",
       "max": 227845,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7c586e8c13d84b4f8ffc020630bb3cbe",
       "value": 227845
      }
     },
     "7b2916c4f0f44964841f91502ac1e724": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7c586e8c13d84b4f8ffc020630bb3cbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "808fff53918347ed9c3c6cceb6472cfa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "80af8bc8cf7b4312bc6cf9b1b7f88b6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "86eb31333d494361a31d5f2453049752": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad058382593c49dd9679f72292f13e05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1458e5cde2746b592c6f0fde398e61e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ad058382593c49dd9679f72292f13e05",
       "placeholder": "​",
       "style": "IPY_MODEL_7b2916c4f0f44964841f91502ac1e724",
       "value": "Downloading: 100%"
      }
     },
     "ba8aa29701b8461e916f561274781924": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c15a120aec2549b491814d4e37b303c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cccbfcc8a6a64691ad4a1667504388d2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d206eeb3d5a547b9b2a2d18b7eabbc52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d81fc517dec8498b856c73d7d8ec0a22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ed4619531f9644a1afebdd47f4f50050",
       "placeholder": "​",
       "style": "IPY_MODEL_00e8f51bfab94006a174348bfa299c7b",
       "value": " 223k/223k [00:00&lt;00:00, 679kB/s]"
      }
     },
     "da72fe1fba434be2a5f9382f7392bd86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4169d7d17ef74e17809015539122e940",
       "placeholder": "​",
       "style": "IPY_MODEL_de410b3cdebc427eb9c439f37ca7e626",
       "value": " 422M/422M [00:15&lt;00:00, 32.2MB/s]"
      }
     },
     "dc6597d6318c4ff7926ff8d6dce72086": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_290bf1ac0031410ba295792e01560623",
        "IPY_MODEL_77de0c4ca0f14bd0a533615ef53cbd83",
        "IPY_MODEL_d81fc517dec8498b856c73d7d8ec0a22"
       ],
       "layout": "IPY_MODEL_3766b78fb5e44941a1c5c126538a800d"
      }
     },
     "de410b3cdebc427eb9c439f37ca7e626": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e4e933581e9d495c927cd6d4737ad59d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_80af8bc8cf7b4312bc6cf9b1b7f88b6a",
       "placeholder": "​",
       "style": "IPY_MODEL_808fff53918347ed9c3c6cceb6472cfa",
       "value": "Downloading: 100%"
      }
     },
     "ec461606d3114ca79862034c9806c968": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ed4619531f9644a1afebdd47f4f50050": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0460f6005b94f6e9eb9e444563f1cf3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b1458e5cde2746b592c6f0fde398e61e",
        "IPY_MODEL_4aadedc0f7ff4f0c846ec784e712f3da",
        "IPY_MODEL_da72fe1fba434be2a5f9382f7392bd86"
       ],
       "layout": "IPY_MODEL_86eb31333d494361a31d5f2453049752"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
